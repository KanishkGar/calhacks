0:03
okay welcome back to 162 everybody um we are going to do the third lecture
0:10
we have uh on scheduling today and um i definitely encourage you to catch up on
0:16
the other lectures if you've gotten behind since the midterm the one of the things we did talk about
0:22
uh last time which i wanted to remind you of was real time scheduling
0:27
and normally when you hear about scheduling in an operating systems class uh you
0:33
often just hear about sort of the standard performance uh sensitive or latency sensitive
0:40
um you know responsiveness and fairness sensitive scheduling algorithms but
0:46
i always like to talk a little bit about real time because real time is different in that um predictability is important
0:53
so rather than what you typically worry about in scheduling here it's far more important to be
0:59
predictable than even to be fast okay because you want to predict with confidence the worst case response time
1:06
and um in real time scheduling uh performance guarantees are often given uh per each task you're
1:13
sort of guaranteed a given deadline uh will be met and um the way you get that guarantee is
1:20
you have to give the scheduler information about what your worst case uh scheduling time uh worst case
1:27
computation time might be um and it could conventional systems we
1:33
talk about performance and you know throughput is important okay and so real time is really about enforcing
1:39
predictability and it's important because for instance talking about things like
1:45
uh hard real time might show up if you're worried about um physical world scenarios how long
1:52
between when you press the brake on a car and when the uh brakes actually engage
1:57
that might be a real-time problem and it's very important that you meet a deadline there because if you don't then
2:02
the user might crash now there is a discussion here about gpu scheduling we probably won't talk about
2:08
that we're mostly talking about regular uh cpu scheduling for now
2:14
the the thing about hard real-time scheduling again is it's really important to meet the deadline
2:20
and this can be a situation where if you don't meet deadlines maybe the car crashes or uh you have a system that's uh in a
2:27
hospital and maybe the patient dies if the real time scheduling's not met and we even introduced something called
2:33
earliest deadline first scheduling last time which is a very common one for doing real-time scheduling um we also sort of
2:39
distinguish between hard and soft real time the key thing about hard real time is it's crucial that you actually meet the
2:45
deadlines and you assume that you don't want to miss any deadlines whereas soft real time is a situation where you want to meet
2:52
deadlines with high probability and typically might be in something like multimedia servers or whatever
2:58
and they're something that like the constant bandwidth server cbs which we didn't talk about last time is
3:04
a variant of earliest deadline first for multimedia all right the other thing that we were
3:10
talking about that i wanted to mention was stride scheduling stride scheduling is uh something that
3:16
we talked about after we talked about lottery scheduling and this was the notion of
3:23
achieving a proportional share of uh scheduling without resorting to the
3:29
type of randomness we talked about in the lottery scheduling and thereby sort of uh overcoming the
3:35
law of small numbers problem where um lottery scheduling really only comes
3:40
out when you uh have long enough tasks that you can can meet that law of small numbers
3:46
uh law of large numbers basically stabilizes it the stride of each job you could think
3:51
of is something like you have a large number divided by the number of tickets and uh for instance w might be 10 000
3:58
and perhaps task a has 100 tickets b has 50 c has 250 and in those instances
4:04
basically uh the strides are for instance 100 for a 200 for b and uh 40 for c
4:11
and uh what is that stride we talked briefly about the fact that sort of every time you get to schedule
4:18
and run for your time slice you uh you add your stride to your counter and those tasks with the
4:26
smallest accumulated stride are the ones that get to run and so as you can imagine the low stride jobs with lots of tickets run
4:33
more often and this is starting to get a way of applying fair queuing to scheduling
4:41
and and basically thereby giving a proportional fraction of the cpu
4:47
okay and really what i talked about a little bit too quickly at the end of lecture because we ran out of time and i
4:53
wanted to repeat here for everybody was this notion of the linux uh
4:59
completely fair scheduler or cfs and this is uh actually in use you're probably
5:05
using it if you have a linux box and the goal here is that each process gets an equal share of the cpu
5:11
so rather than talking about priority scheduling or or uh talking about round robin
5:17
scheduling or some of the other ones we were talking about which don't tie the schedule directly to the cpu
5:24
cfs like stride scheduling ties the amount of execution time you get to the cpu and so
5:31
as a simple example we'll get to more complicated ones in a second here the idea here that n
5:36
threads are running simultaneously you have this model as if the cpu
5:41
were subdivided into n pieces and somehow we were able to get n
5:46
pieces of the cpu to each of the n threads and if you could somehow do that then
5:52
the threads would run uh at exactly one over nth of the time and they'd all get an equal fraction of
5:57
the cpu and everybody would be happy okay and so the model is something like
6:02
simultaneous multi-threading or hyper threading where each thread gets one over n of the cycles
6:09
of course in general you can't do this uh yeah hyper threading maybe lets you do that a little bit with
6:14
one or two threads but certainly not a big n and so what we need to do is figure out how to
6:20
approximate this idea that every thread gets one over nth of the cpu
6:26
but without having that ability to really subdivide those cycles and so of course the operating system
6:33
gives out full speed cycles and so we have to use something other than the
6:38
some way of keeping the threads in sync so they sort of get on average one over n and that's really the basic idea here
6:45
which is we're going to track cpu time per thread and schedule the threads to match up an
6:50
average rate of execution and so you could look at this i mean in this previous figure what i had here was
6:57
1 over n of the cycles are given to each thread and so they all kind of
7:03
progress at the same time okay in this newer idea here the threads of course when they're
7:09
running are running fast they're faster than one over nth of the speed but we don't run them all the time and
7:16
if we when we stop we can take a look at thread one two and three and notice uh to make a scheduling
7:22
decision here that thread two is behind in its average amount of cycles
7:27
and so we'll choose to schedule that one next and so it's uh we've sort of keep the the heads of the
7:33
threads uh running at the same speed on average so we choose the thread here with the
7:39
minimum cpu time total and this is very closely related to fair cueing as a general idea if you're
7:45
familiar with that from networking okay and uh if we do that so what we're just to
7:52
be clear what we're doing is whenever the thread gets to run we're counting its total cycles and then
7:59
when we stop uh we put it back in the scheduling heap and then we pick
8:04
the thread that has the the least number of cycles so far and we keep doing this
8:09
in a way so that on average we get the same uh rate of execution between
8:15
all of the threads okay and you could imagine if you know if you remember your 61b um
8:22
ideas is that we probably want to keep like scheduling queue so we just put the threads in the heap and then the one at the top is the
8:29
lowest uh has the lowest number of uh total cycles
8:34
and it's the one that we schedule for next all right questions so this is
8:41
we're going after rate of execution here rather than those other metrics that we were going at before like uh
8:48
you know letting it just run for a little while and then switching it out after time expires
8:54
and i'll show you in a moment how we can now use this to give us something like priorities but in a way that still
9:01
maintains this notion of rate of execution rather than strict priorities
9:08
okay questions okay so sleeping threads
9:16
of course don't advance their cpu time so what's interesting about this is that when they wake up and they're
9:21
ready to run they're way behind and so they get selected to execute first and as a result we get this
9:28
interactivity idea automatically uh think of this in in contrast to the
9:33
o1 scheduler uh and the cfs has any pr concept of priority yes just give me a second i'll
9:40
get to that but if you remember the o1 uh scheduler the idea was we had some really
9:45
complicated heuristics uh that would adjust priorities based on
9:50
how much interactivity we thought or how short the burst time seemed to be to try to make sure that things that had
9:56
really short burst times and might be likely to be um interactive tasks would get higher
10:04
priority and get to run as soon as they became runnable here we get this automatically just because
10:10
we're trying to give the same rate to everybody and if a threat is sleeping it's not achieving its rate so when it wakes up
10:16
suddenly it's got the cpu okay so this is beginnings of why this was so appealing
10:22
and why basically linus and others uh completely threw out the o1 scheduler for cfs
10:28
because o1 had gotten way too complicated so um so cfs
10:35
has some nice properties uh to it but we we still want to worry about a few things we talked about for instance
10:41
starvation last time and responsiveness and so in addition to trying to be fair
10:46
about the rate of execution we certainly want low response time to make sure that no thread left behind
10:53
right and so starvation freedom might be another way to look at that and so we want to make sure everybody gets to run at least a
10:59
little bit uh if you recall when we were talking about multi-level queuing
11:04
there was this worry that the q the uh thread sitting at the very bottom there in the lowest level q might never get to
11:11
run so what cfs does is it actually makes sure that everybody gets to run a
11:17
little bit and so it has something called the target latency which is a period of time over which every process gets to run a little bit
11:25
okay and so call the quanta target latency over n in this case that means that we make
11:30
sure that every thread runs one over nth of the time and that makes sure from a time standpoint we
11:36
still have uh the ability to [Music] you know be sure we're going to run now
11:43
so far it sounds like we're moving our way back into round robin but just hold off as soon as we get to
11:48
priorities you'll see how this is fundamentally different so for instance a target latency of 20
11:54
milliseconds is not out of the question for cfs if you got four processes running then each process gets filled
12:00
five milliseconds time slice okay and the problem that you might think
12:06
here is if we have a 20 millisecond target latency but 200 processes then this all falls apart and so cfs
12:13
does have some outs um call it uh a way to get by
12:19
this high overhead case all right and that's going to be that we're going to have a minimum quanta time we never
12:25
want our overhead to get so high that for instance 0.1 milliseconds is essentially what i told you a context
12:31
switch time can be in some circumstances so it would be really bad if we switched every contact switch time
12:38
okay and so that's basically a throughput metric and so cfs has something called minimum
12:44
granularity which is the minimum length of any time slice and so the target latency uh 20
12:51
milliseconds minimum granularity is one millisecond that says in this case of 200 processes
12:57
uh we basically don't run anybody any shorter than a millisecond
13:02
and so when you're when you have so many things running that you hit the minimum granularity that's
13:07
typically when the properties of css the cfs uh start to fall apart a little okay but
13:15
just so you know there is this minimum granularity piece as well okay priorities now um
13:23
as those of you have used linux recently it still has priorities i wanted to tell you about what priority
13:29
in unix typically is and that's the nice value so the inner dust the industrial operating systems in the 60s and 70s
13:36
gave you an actual priority that you could set directly when berkeley unix was kind of working
13:43
on priority they decided to call this nice instead niceness okay and so
13:48
when we were talking about the o1 scheduler we mentioned the fact that there were 40 priorities for the user
13:54
those are actually called nice values and they range from minus 20 to 19 there's 40 of them in there
14:01
and negative values are not nice uh positive values are nice
14:06
and something that's more negative than another one gets higher priority okay so even if you were to look at
14:12
priority 19 versus 18 the thing with the nice value of 18 is running with a slightly higher priority
14:18
okay and um so for instance what you would do is you could start a job and then you
14:24
could run nice on it and uh so if you wanted to let your friends get a little more time you might do nice
14:30
on your job and that would raise the niceness value and however only the
14:36
root user is allowed to lower nice uh the regular users are allowed to raise
14:42
the nice values now uh as i mentioned the scheduler puts
14:47
higher nice values or lower priority to sleep more and in the o1 schedule this actually translated fairly directly to priority
14:54
if you remember i showed you that there were 140 total priorities in the 01 scheduler
14:59
the the uh the highest hundred of them were for what was called real time scheduling
15:05
okay and the uh the lower 40 were for these nice values okay but how does this
15:12
translate to cfs so cfs was a drop-in replacement for the o1 scheduler so clearly there was some notion of
15:19
niceness that must have been there and priority is certainly useful because certain certain things are
15:24
higher priority than others but this idea that cfs is a fair
15:29
queuing type scheduler says that there must be something um a little different here and because
15:36
this is not strict priority and so the idea here is that you're going to change the rate of execution based on priority you're
15:44
not going to say that higher priority always runs over lower priority but instead higher
15:49
priority has a higher rate of execution than lower priority okay so how does this work
15:57
so cfs as i've shown you so far isn't really all that different from
16:03
round robin okay because i kind of said you know you get one over nth of the cpu you get a quantum that's
16:09
uh one over nth of the target latency and so it sounds like i just renamed round robin but in fact i didn't
16:17
okay what i did was um i only showed you the uninteresting case where everybody has the same
16:22
priority but what if we want to give more cpu to some and less than others
16:28
what we're going to do is change the rate okay and so we're going to use weights for that so what i showed you earlier
16:35
was one in which the basic uh quanta was uh everybody got target latency time
16:41
over n and that was the this basic equal shares okay a weighted share is something where
16:48
every thread has a weight and then what we do is we take the current weight divided by the sum of all
16:54
the weights to find out what fraction of the total weights the thread has times target latency and that tells me
16:59
my quanta okay so now i'm adjusting the time
17:04
that i'm allowed to run based on a base to some extent on target latency
17:12
and we're going to reuse the nice values to reflect the share rather than the priority so cfs uses nice values to scale the
17:20
weights but it does so exponentially okay now this looks messy but it's not bad so just hear me out
17:26
so the idea is that the weight is a 1024 divided by
17:32
1.25 to the nice value so what does this say this says that positive nice values
17:37
uh have lower weights than negative nice values okay so a high weight basically um
17:46
is going to be something that uh has a so as you see here so high nice value
17:51
has a low weight and a low nice value has a high weight okay and so two cpu tests separated by
17:57
nice value of five which you find is the one with the lower nice value has three times the weight of the one with the
18:02
higher and that's doesn't matter where it is so if you have 19 versus 14 or zero versus minus five
18:10
you're still going to get the the same proportional difference there okay and now
18:16
we're going to use virtual runtime instead of cpu time okay and the virtual runtime
18:22
why 1024 because they did the the thing to realize here is it
18:28
doesn't matter what the number 1024 is we could put any number we want here because the only way we use it
18:34
is uh with the same uh number in the numerator and denominator okay and so um yeah this is more about
18:41
wanting integers than it is about anything else okay but the thousand twenty-fours end up cancelling out in this weighted share
18:47
number so the actual number is more of a uh convenience for the number of bits you have in your
18:53
weight than anything else all right now um so
18:59
just to give some of these numbers uh to you just for the heck of it if you have a target latency of 20 milliseconds
19:06
minimum granularity of a millisecond and you have two cpu bound threads which
19:11
are always running then a um might have a weight of uh one and b might have a weight of four
19:17
how does that work well with the target what latency of 20 milliseconds then these two weights which would come
19:23
from that exponential factor i gave you earlier means that the time slice for a might be 4 and for b might be 16. so
19:30
notice b has a bigger time slice than a okay and they're in the ratio 1 to 4.
19:37
now so let's go back now to the fair cueing aspect
19:42
okay so fair cueing how do we fit the rate of execution
19:50
back into the picture here because so far we're talking about time of execution but that isn't rate so to fit the rate of execution back in
19:56
the picture what we want is we want somehow to give a slightly faster cpu to things with
20:02
higher weight okay and so if you look here um uh
20:08
here's an example where we want to give say more time to the higher weight than the lower weight one but to do our
20:16
our cfs scheduling what we're going to do is we want to schedule this in a way
20:21
that we can put everybody in the same heap no matter what their weight is and always pick the one that has the lowest
20:27
uh amount of time so far and so we do is we schedule virtual time instead of
20:34
real time so this is kind of cool so listen to me for a sec here so what you do is um for higher weight
20:41
the virtual run time increases more slowly and for lower weight it increases more quickly so if you
20:47
think about that a higher weight i let the cpu run for a second but i only will
20:52
look at say a quarter of a second worth of virtual time whereas for the lower weight one here if
20:58
i run for a second i will register a second of virtual time if i put those together
21:03
into my virtual cpu and i make sure that virtual time always advances at the same
21:08
rate then voila now the ones with the higher weights get to run
21:14
more time than the ones with the lower rates and it does it in this very simple scheduling uh idea here where i just want to make
21:20
sure every thread has the same virtual cpu time okay so scheduler's decisions are based
21:25
on virtual cpu time it turns out you you take the amount of time you just ran when you give up the
21:31
cpu you divide it by your weight and you register that virtual time and then you put yourself back in the heap
21:37
and uh turns out they use a red black tree uh to do this uh which is a convenient heap that i'm
21:42
sure you've learned about it basically you can always find the next thread to run in o1 because it's at
21:48
the the top of the heap and then you run it for a while and you do this same trick okay and now by the way the question
21:56
here that's in the chat is does this assume that um every process has only one thread so this
22:01
scheduling decision is based per thread not per process right now okay and if you
22:07
wanted to have some tie to the processes then what you would do is you would adjust
22:12
uh their total weights to reflect that okay so you can you can
22:17
basically scale their weights to do some process-based scheduling if that's what you were desiring
22:23
all right now the in a contrast to the o1 scheduler which every task that the o1 scheduler was
22:30
doing was uh independent of the number of threads because we're using a heap the
22:35
scheduling time here is order log n but log n isn't too bad and the net result though is this
22:40
incredibly simple schedule okay so notice that priorities are reflected by
22:46
um a greater fraction of the cpu cycles or a greater rate of execution
22:52
that thing about interactivity just happens because when you go to sleep and you wake up your virtual time is behind
22:58
and so you get to run right away and so all of the really complex heuristics that were
23:03
in the o1 scheduler have been replaced by this very simple idea of scheduling virtual time
23:09
all right any questions so by the way if a thread schedule
23:14
spawns too many or if a process spawns too many threads then then the operating system can make a decision about whether to shut them down
23:21
or not okay questions
23:28
so this is a fair queuing with execution rate mechanism of scheduling all right
23:37
so just to close out this scheduling idea so i wanted to go through the cfs in a little bit more detail because this
23:44
is um an actual scheduler that you're probably using now that works pretty well it's
23:49
not a real-time scheduler because it's working with rates of execution so if you want a real-time scheduler
23:55
you could install for instance uh you know earliest deadline first scheduler on linux and you could use that um
24:04
so is there a cap on how much interactivity boost that a long-running thread can have yes so if you get too far behind then there
24:10
is there is a little bit of a reset that goes on there but uh um for short for short shutdowns it
24:16
doesn't happen that way okay now um if you care about
24:21
cpu throughput you might use first come first serve because that's the one that uses the things in the most efficient way if you
24:28
care about average response time then you might want some approximation to srtf because remember srtf
24:34
is optimal io throughput excuse me you might use an srtf approximation
24:40
fairness well you might use cfs or if you're caring about the wait time to get to cpu perhaps you'd use round robin
24:48
if you're interested in meeting deadlines you probably use edf one thing we didn't talk about in this
24:53
class is rate monotonic scheduling which is a type of scheduling that's not as optimal as edf but you can actually do
25:00
rate monotonic scheduling with a strict priority scheduler like we talked about
25:05
is the top hundred priorities in linux and so you might do that instead of edf
25:11
if you're interested in favoring important tasks you might use a strict priority scheduler okay all right so a final word on
25:19
scheduling before we move on to to another topic is when do the details of the scheduling policy and fairness
25:25
really matter when there aren't enough resources to go around so
25:30
everything we've been talking about for scheduling is all about how do you choose to divide
25:36
up your resources among a bunch of shared threads if you didn't care about resources you
25:42
wouldn't have to schedule um or if you didn't care about uh you know if you didn't have more than if you
25:47
had one thread for instance that's what i meant to say you wouldn't have to schedule okay so when there aren't enough resources to go
25:53
around your scheduling policy might start to get really important okay
25:59
and that's when you really have to be careful about your scheduler okay when should you just buy a faster
26:05
computer so it could be the case that your resources are so scarce and you have so many things you
26:11
have to run that your computer is just not fast enough and you know this goes with pretty much
26:17
everything uh when might you need another network link or expand your highway or any
26:22
any number of questions around the rates of uh restricted resources are all about
26:28
how do you schedule and then if scheduling starts to fail when do you buy bigger faster
26:35
larger things okay um one approach is you buy it when it's
26:40
going to pay for itself and repro improved response time um perhaps you're paying for the uh for worse response time and reduced
26:47
productivity customers being unhappy et cetera you might think you should buy a faster
26:54
something when something's utilized 100 percent because then you know you you can't utilize it anymore
27:01
but i want to tell you that running anything at 100 is always bad okay um
27:08
you as an engineer should know that it's never want to run anything uh at 100 if there's any randomness in
27:15
the system at all and the reason is that you start getting this queuing behavior like i've shown you in the curve here now we're going to
27:21
talk about cueing theory in more depth uh in a little in a few weeks
27:26
actually we may talk a little bit about it next week but in general you see a curve that looks
27:32
like this with utilization on the x-axis something like response time on the y-axis and a
27:38
a non-linear curve that starts out with a linear section in the low part but then
27:44
rapidly starts rising okay and then when you're looking at the the regular models
27:50
that um aren't realistic but totally mathematical this uh this high end near 100 goes to infinity
27:57
of course we know nothing goes to infinity in real life but it always goes pretty high okay and
28:02
so 100 is definitely not the time at which you want to buy something because you're already seeing this huge super linear
28:09
increase in response time so your customers have already left you right so an interesting application
28:16
we'll tell you where the curve comes from in another lecture but here one thing might be to say as long as i'm
28:23
in the linear portion of utilization things are basically okay okay the
28:28
moment i start getting to the point where it's super linear and things are going up faster than they were in the linear section that's when i start
28:35
uh to worry about my resources so right around the knee of the curve is usually a good place to um consider buying something new
28:44
okay and just to give you another instance of 100 being a bad idea if you know that a
28:49
bridge can handle some maximum weight say call it you know 200 tons
28:56
you do not want to be running at 100 on that bridge because you know that any sort of randomness is going to
29:01
take you over the edge right you want to be running down in the linear place where the bridge is behaving normally
29:06
okay all right good questions
29:13
we'll tell you why this curve is super linear in a couple of lectures
29:20
okay um so i actually had this as still
29:27
grading when i did these slides earlier today but um i believe the grading is pretty close to done um so we'll get those out to you there's
29:34
also um i know people have been waiting for the bins those bins are out there's going to be a little bit those
29:41
bins represent final total points as they do in other classes but
29:46
for midterms there is a an offset that you can use with the midterm from historical
29:52
data that will let you interpret kind of how you did on the midterm um and we'll explain that uh later in a
29:59
post but i will say that having we graded this and this midterm was clearly too long and i
30:06
apologize for that it was definitely definitely hard harder than i think we were expecting so
30:13
i guess we'll figure that out for the next one so my apologies there
30:19
the other thing is group evaluations oh and just to cap this off i believe you'll be
30:25
seeing the um release of great scope grades in in either tonight or maybe
30:31
even uh early tomorrow but very soon and that'll be the process then you can start
30:36
um putting in uh grade regrade requests and so on um group evaluations uh are coming up
30:44
for project one in fact they may have been mailed out today or they will be tomorrow at the latest
30:49
every person in the evaluations are going to get 20 points per other partner
30:54
which you can hand out as you wish no points to yourself okay every term i have to say no points
30:59
to yourself so this is not about saying well i've got four people in my group 20 times 4 is 80. i'm going to give 80
31:07
points to myself okay that doesn't work that way the way it works is you have three other
31:13
partners 3 times 20 is 60 and you can hand those 60 points out anyway to your other
31:18
partners they're going to evaluate you okay and um the reason we do this and by the way
31:25
your tas are going to moderate what's uh what's being said here this is just one piece of information that we use to
31:32
figure out how things are going but in principle projects are a zero-sum
31:37
game and you have to participate in your group okay and there are some of you that seem to have fallen off the earth
31:43
and aren't responding to email if you really don't participate at all
31:49
and we have that documented in various ways then it's possible that some of your points
31:55
may end up going to your partners instead of to you so this doesn't happen often but it's a
32:01
way for us to really uh reward project members that um are working and uh have non-working
32:10
team members okay so please try to try to work um make sure that if there are any group
32:16
dynamic issues that your ta knows um and i think i offered that i'd be
32:21
more than happy to sit down with groups to talk about ways of collaborating if that helps but
32:28
make sure your ta knows any issues that you might be having with your group and let's see if we can make projects
32:35
two and three even better um so are the point distributions per
32:40
person anonymous so um the point distributions are in fact not uh handed out at all so those are
32:48
purely for our information um they none of your team members know how you graded them and they don't
32:54
um and you don't know how they graded you um but you're gonna uh talk to your tas and
33:00
their tas will have a good idea how you're doing as well okay um the the other thing uh just to
33:08
say about this is you know if you were 100 happy with your group members you could give your other three partners
33:15
or whatever 20 points each and that would be an example of a uh fully um a very happy person
33:22
uh with the rest of their group okay the other thing i mentioned was this notion of a group coffee hour
33:28
uh look for opportunities i think in maybe the same email that we send out either group
33:34
evaluations or our uh how are we doing a third of the way through the term
33:39
uh survey um we're going to tell you how to basically give it get maybe get extra points for screenshots
33:46
of you uh with uh your other team members on zoom
33:51
you know thumbs up or beverage of choice or whatever we're going to call these group coffee hours okay and don't forget turn cameras on
34:00
for discussion sessions uh if if at all possible all right
34:06
that was a long administrivia um i realize it's uh really rough being in a fully virtual
34:12
term and you know a third of the way through the term this is the point at which things start getting uh they seem hard and uh
34:20
you people you sort of hit a a slow point but let's let's get our excitement back up and get
34:25
moving and i know we have a bunch of really exciting topics still in the class so and i apologize that
34:32
that midterm was too long i think we haven't fully figured out how to deal with
34:37
virtual virtual midterms yet all right
34:44
good so let's change topics so let's talk about deadlock uh i like
34:50
to think of this as a deadly type of starvation so starvation as we've been talking about with scheduling
34:56
uh as as our main instance certainly last time is a situation where a thread waits
35:02
indefinitely an example might be a low priority thread waiting for resources constantly
35:07
in use by a high priority thread of course the principle resource being cpu but other things can be there too
35:16
this is a situation that could potentially resolve itself as soon as all the high priority threads are gone
35:21
so it isn't a permanent scenario but it certainly might be annoying and it might be um you know not what you want because
35:28
your thread's not running deadlock on the other hand is an unresolvable situation that's a
35:35
starvation situation and it involves a circular waiting for resources so if you look here
35:41
we have a situation where thread a is waiting for resource 2 but resource 2 is owned by thread b
35:48
and thread b is waiting for resource 1 but resource 1 is owned by thread a so here's a cycle
35:54
and as a result of this cycle both thread a and b are sleeping and will never wake up
36:01
okay because you know a will never get notified that resource two is ready and b will never get notified that resource
36:08
one is ready and uh nobody resolves itself and uh nobody's
36:13
happy okay so notice that deadlock is a type of starvation but not vice versa
36:20
okay and again starvations can end they don't have to but they can deadlocks can't there's no way to fix
36:26
this cycle i'm showing you here without fundamentally doing something drastic like thread a killing
36:32
it off then thread b could run or um i don't know trying to figure out how to temporarily
36:38
take a resource away from somebody and then give it back okay and both of those situations are usually bad
36:44
just randomly killing a thread probably isn't what you want to do and randomly taking a resource away from
36:50
somebody probably gives you bad behavior okay so um what's a good example of a resource
36:57
other than locks and semaphores so we'll talk about uh memory uh you know disk blocks
37:03
pick any resource you like a queue um you know think anything that uh
37:09
you might wait for is a situation where you might be in a pr uh run into
37:14
problems okay now um you know another example could be
37:22
that you're waiting for a particular cpu in some special machine that's uh
37:27
attached in a way to some hardware that other cpus aren't attached to
37:32
that could be an important resource that you're waiting for so pretty much anything that you need to complete your task that
37:39
might need to be exclusively owned counts as a resource here okay
37:44
did that help now here's the simplest example uh here's a
37:51
bridge we have a lot of these in california um i was just uh out driving the roads last weekend and
37:58
uh encountered one road where there was like three of these single lane bridges uh all
38:04
because parts of the road had washed out and uh they never got fixed from the last rainy season so that's unfortunate
38:10
but um you could imagine that uh this might be a source of deadlock under
38:17
some circumstances so for instance you could view each segment of the road as a resource
38:22
car has to own the resource that's under it of course and they may need to acquire the segment they're moving into in order
38:29
to make any progress okay so um for instance if you have a
38:34
bridge and let's just divide it in two halves you have to acquire both halves and traffic only in one direction at a
38:40
time is clearly going to be required for that so here's a here's a bridge situation
38:46
where there's two halves we have two cars that are on each half and we have a bad situation
38:54
here because the two cars are meeting in the middle and can't make any progress
39:00
okay and i've shown you here a cycle you know we have the minivan it's trying to get the eastern half of
39:05
the bridge and we have the uh race cars trying to get the western half of the bridge the minivan owns the western half
39:11
because it's on it and the and the race car owns the eastern half because it's on it and we have a cycle
39:18
okay and um how do we resolve this deadlock well if we want to resolve the deadlock in a
39:23
way that's uh reasonable one of the cars has to back up amusingly
39:29
enough if you get two people that are unwilling to back up then you get a long term honking going on the other thing to note
39:37
by the way is because of the ownership of resources prior it's possible that for instance in
39:43
order for the green car to back up other cars have to back up and so there may be a whole chain of resources that
39:49
have to be uh relinquished and reacquired only in order to undo that deadlock
39:57
okay those of you that might have taken a database course like 168 or something like that might recognize that um
40:04
164 might recognize the situation as some sort of undo or transaction abort okay
40:12
186 that's what i meant sorry i'm being i'm being swapped tonight uh so the other thing that can show up
40:21
in this scenario is starvation if for some reason one direction say uh you know east
40:28
or west to east is just going so fast that no other car gets in that's actually a type of starvation here not
40:34
deadlock because um you know as soon as there's no more of that traffic then the
40:39
other traffic can go all right so let's look at um deadlock with locks
40:47
here since uh this seems like the simplest thing to start looking at so here here are two threads um
40:55
this is a situation where the municipality might need another lane right well as i mentioned on that road i was on literally there
41:02
couldn't be another lane because it was it was uh washed out and there was um just those jersey barriers around
41:09
there to prevent you from going into the creek so um i would say the local municipality uh
41:15
wasn't able to fix it so here's a situation where thread a and thread b um look
41:22
as follows they both are have mutex x and y but thread a doesn't acquire uh of x and
41:29
an acquire of y it does some stuff then it releases y and then releases x thread b
41:34
on the other hand acquires y and then acquires x does some stuff releases x releases y
41:41
so this lock pattern seems simple it seems like something you
41:47
could write by accident if you weren't thinking about it because you got two resources x and y you need them both uh you write one in
41:54
one order and the other in the other order and um the problem with this is that
42:00
this is a non-deterministic failure okay and there's nothing worse than writing something that fails
42:06
non-deterministically because you can't reproduce it to start with i'm sure some of you have started to run into problems like that in
42:13
uh in the code that you're writing and um you know and the other worst
42:20
thing is it's going to occur at the worst possible time now if you remember when i was telling you about
42:26
the murphy's law scheduler or the malicious scheduler view this is a situation where
42:32
the scheduler will find the bad situation and they'll do it at the worst possible time
42:38
now let me show you a little bit about the unlucky case so fred a acquires x thread b acquires y now
42:45
notice the interleaving going on here right thread a tries to acquire a y but it's
42:51
stalled because it y is acquired by b thread b tries to acquire x but it's stalled and now
42:59
the rest of that code never runs okay so this is a deadlock and if you notice here so thread a is
43:04
kind of waiting for mutex y and thread b is waiting for mutex x and neither of these are going to give
43:11
it up and so basically we are stalled okay
43:18
neither thread gets to run we've got deadlock but let's look at the lucky case oops sorry
43:24
about threading b so the lucky case here thread a acquires x and y then thread b comes along and acquires y
43:32
uh it tries to acquire y but notice that it's uh stuck okay and then thread a releases y
43:38
and releases x then b finally gets to acquire y then it acquires x and it runs
43:44
and the schedule doesn't trigger deadlock and if you think about what's involved in getting that exact deadlock case to
43:51
happen well the scheduler has to line up at exactly the wrong time with this previous case
43:57
here to get the deadlock most of the time it won't happen and you'll get the lucky
44:04
case so here you are you ship something to customers and you get a call at 3 37 in the
44:11
morning because the thing is deadlocked because most of the time you're seeing the lucky case but you didn't see the
44:17
unlucky case when you were testing okay and the larger amount of code
44:23
that isn't in your lock uh case so like here you know we have a few
44:29
instructions here doing locking but we have a lot of code maybe in the critical section and a lot of code outside the critical
44:35
section you know it's from a probability standpoint it's just not a high probability event but boy when it
44:41
happens you are toast okay
44:48
questions all right everybody good
44:55
now let me show you another case so
45:02
here's a here's another circular dependency that's a little bit different but it's similar okay and i'll tell you
45:07
why i'm calling this a wormhole rooted network in a moment but for now this is there is some trains here they're long
45:12
trains there's a little tiny train over there too but these long trains stretch uh for a
45:18
while a while since they're long and what we've got here is each train is trying to turn
45:23
right so this uh this eastern facing train's trying to go south the south trains trying to go west the west train
45:30
is trying to go north the north strain is trying to go east and they're blocked because the resource
45:36
they need which is for instance this west east train is trying to grab that segment immediately after
45:44
the turn but it can't because there's a train in it okay and this is actually a very similar problem to what you get in a
45:50
multi-processor network okay so this is a situation where um
45:56
where you've got basically a wormhole rooted network with messages that trail through the
46:02
network like a worm so instead of trains what we've got is we've got a routing flit at the head
46:07
and then the body of the messages kind of stretch out all the way back to the source of the messages
46:13
okay so that's called wormhole rooted networking because it looks like a worm and it's rooted as that worm all
46:20
the way through the network okay and here we've just developed a deadlock okay so how do we fix this well what you
46:28
do in the network case this may not be as practical in a train except maybe in the metropolitan area is you you make a grid
46:36
that extends in all directions and then you force an ordering of the channels okay and the protocol will be
46:42
you always go east-west first and then north-south okay so what we've just done is we've
46:49
disallowed by this rule these two uh parts of the turn so this
46:56
red turn here and this red turn there so you're not allowed to go north first and then east you're also
47:01
not allowed to go south first and then west and by disallowing those two turns
47:08
you will never get deadlocked because you can't fundamentally get a cycle out of it in fact you can even write a proof that shows that this
47:15
network has no deadlocks in it because a deadlock would require a cycle and a cycle would always require
47:22
at least one of these disallowed turns okay now again this is not as practical
47:28
in a trained network but certainly in a network network if you have a mesh what you can say is i always have to
47:34
route east west first and then north south and as a result you can end up with no deadlocks okay
47:41
questions
47:46
all right now by the way this is a real xy routing is a real thing or you look
47:51
up dimension ordered routing there are there are real networks that
47:56
behave that way including uh you know the interior networks that are
48:01
part of the intel chips so this is a this is a real thing and it's a way of avoiding deadlock so it's kind of nice
48:07
because you can avoid it mathematically other types of deadlocks there are many of them right so threads block waiting
48:13
for resources like locks and terminals and printer printers and drives and memory
48:20
threads might be blocked waiting for other threads like pipes and sockets
48:25
you can deadlock pretty much on anything like that and all it requires is getting some sort of cycle involved
48:31
okay so we might want to figure out a little bit about how to avoid these kind
48:36
of deadlocks okay so here's an example of one with space right so here thread a alec is going to
48:44
do an allocator weight one megabyte and then another megabyte and then free free and thread does the
48:49
same thing well if there's only two megabytes total of space you can imagine that a gets a megabyte then b gets a megabyte
48:56
and uh we're now deadlocked in just the same cycle as before but it looks a little
49:02
different okay and we'll talk about how to think about cycles that uh
49:07
that have resources where there's multiple equivalent pieces of the same resource in a little bit so
49:15
in order to move our way along this let's talk about what i like to call the dining lawyers problem
49:21
so we have five chopsticks and five lawyers okay and a really cheap restaurant and
49:27
it's a free-for-all so what we do is we put one chopstick in between each lawyer
49:33
okay and the lawyers are going to grab and by the way nothing against lawyers this is just the
49:39
example here but you need two chopsticks to eat and um if everybody grabs the chopstick
49:45
on their right we now have deadlock because nobody can can eat okay so that's a
49:51
that's a deadlock it's a larger cycle than just uh you know two resources and two threads
49:58
but it's still a deadlock because there's a cycle so how do you fix the deadlock well you could make one of them give up a
50:03
chopstick and eventually everybody gets a chance to eat oh and by the way this is such a cheap restaurant that
50:10
you have to share the chopsticks after they've been used and you put them down so perhaps during a pandemic you
50:15
wouldn't want to do this solution um how do you prevent a deadlock well that's more interesting right
50:20
so the way you might prevent a deadlock here is to never let a lawyer take the last chopstick if no hungry
50:26
lawyer has two chopsticks afterwards now wait a minute what does that mean if you never let a lawyer take the last
50:32
chopstick if as a result of taking that no other lawyer has two chopsticks
50:38
then you know that there's always somebody that can finish dinner and lay down their two chopsticks and then let somebody else go forward
50:45
okay so there is a solution to this that involves uh looking ahead that
50:51
maybe we can formalize in some way okay but to do that we need to talk a
50:57
little bit more about deadlock so what is required what's the minimum requirements to run into a deadlock
51:04
well first and foremost mutual exclusion is a requirement so that says that we have resources
51:10
that can be possessed exclusively by a thread such that no other thread can use them
51:18
okay so remember we've been talking about mutual exclusion as a way of keeping a
51:26
multiple threads out from the middle of a particular block of code this is the same idea but this is for
51:32
general resources we're saying that we have resources that can be mutually held onto by one thread
51:38
and requested by another but not acquired until the first thread is done with them that's mutual exclusion
51:44
the second is this idea of hold and weight which says that if a thread has multiple resources it's already acquired
51:51
and it's waiting to acquire another one then it's going to hold on to all the resources that it's got
51:57
so it's what happens is it grab grab grab resources tries to grab the next one and it can't
52:04
but it's gonna hold on to all the other ones okay so you need to not only have mutual exclusion of resources but you gotta be able to
52:10
uh have a situation where you hold them and wait on them there also needs to be a situation with
52:15
no preemption so not only do you hold resources uh while you're waiting for other ones but it's not possible to take a resource
52:22
away from somebody okay and that's kind of like if you think about the bridge example um what would be uh what would be a
52:29
preemption case there well that would be godzilla comes by grabs one of the cars that's honking
52:34
and uh tosses it into the other valley and now we've just broken the deadlock okay so we're assuming that something
52:40
like that can't happen i'm assuming you all know who godzilla is but perhaps i'm dating myself on that
52:47
and the third thing there's you need or four things excuse me is you need to have a circular weight
52:52
where there exists some set of threads that are waiting t1 through n where t1
52:58
is waiting for something held by t2 t2 is waiting for something held by t3 etc
53:03
tn is waiting for something held by t1 etc all right and now as a result
53:10
um we uh we have a cycle okay if you don't have a cycle of
53:16
waiting there is no deadlock now what i want to make sure i'm clear on here is
53:21
you can have all these things and not have deadlock but if you don't have one of these things you don't have deadlock
53:28
okay so these are minimum requirements but they're not sufficient they're just
53:34
they're just necessary okay so we're getting somewhere and if you were to think through all of
53:40
the examples of deadlock i've shown so far it had all of these properties to it so
53:46
let's talk about how to detect deadlock and to do that we're going to build a resource allocation graph so here's our model we have threads
53:53
which are going to be circles with t sub something in them we're going to have resources which are
53:59
going to be rectangles and uh we'll call them r1 r2 etc and notice the number of dots in the
54:05
rectangle represents the number of instances of that resource in the system so these
54:12
are all equivalent excuse me so in the case of memory remember that example i showed you a little bit ago
54:18
where we were allocating one megabyte and then one megabyte and then one megabyte each megabyte is equivalent in
54:23
those instances so we would build that as a rectangle with a bunch of dots representing all
54:28
the equivalent megabytes and we'd call that a resource okay the resources which were mutexes or locks
54:35
that we were talking about earlier might be an example here of a square with a single dot in it okay
54:42
every thread is going to utilize a resource by first requesting it then using it then
54:48
releasing it okay and this notion of request use release is kind of that that idea of mutual exclusion where
54:55
between request and release if i'm in the use phase uh nobody else can use that particular resource
55:02
but that means a particular dot is now used not all of the resources that are equivalent
55:09
okay so our resource allocation graph is very simple okay it's a it's partitioned into two
55:16
types of nodes t nodes and r nodes and we're going to build that graph where there's a request edge uh which is
55:23
sort of t one to r j and that basically says that thread one wants resource j
55:30
or an assignment edge r j to t one t i which basically says that r j is
55:36
owned by t i okay and that's going to build a graph for us
55:41
and then we're going to go through that graph and figure out whether we have deadlock okay i have some examples here so
55:48
remember the model is request edge and assignment edges and so here's a simple example
55:54
so here's an example of threads one two and three thread one uh is requesting resource one that's
56:02
what this uh request edge looks like here we have an assignment edge r1 is owned by t2 okay
56:10
so this is and everybody see that so here's an instance where r4 there's three possibilities there but only one
56:16
of them is currently owned by t3 okay everybody with me so far now once
56:22
we have a graph like this then we can do graph operations on it and very quickly decide whether it's deadlocked
56:27
so for instance here's an example of a graph with a deadlock now it's not your simple deadlock
56:33
but if you look here it's got t1 is waiting for r1 but r1 is owned by t2
56:41
r3 is owned by t1 one of the instances and the other instance of r3 is owned by t2
56:46
t2 is waiting for r2 but r2 is waiting for t is owned by t3 and finally t3
56:52
is uh waiting for r3 and if you look at this scenario this is an unresolvable situation where
56:59
there's no way that any of the threads can advance and make forward progress
57:05
okay now so good question uh so the question was
57:12
so a cycle leads to deadlock no a deadlock needs a cycle very important
57:18
here a cycle is merely necessary for deadlock not sufficient so for instance good question i i
57:25
clearly paid him for uh to ask that question if you look here here's an example of a cycle but no
57:30
deadlock so notice that t1 is waiting for r1 r1 is owned by t3 one of our ones is owned
57:36
by t3 t3 is waiting for r2 one of the r2s is owned by t1 so there's a cycle here
57:43
but what we also see is that if t4 finishes it'll free up an r2 and then t3
57:49
can get what it needs and it'll finish and then it can free up in r1 and then t1 can finish so
57:55
just because you have a cycle doesn't mean you have a deadlock but if you have a deadlock you know you have a cycle
58:02
all right good so now we have we're armed and we can
58:08
figure out how to detect deadlock right so here's a simple algorithm and what it the key thing about this
58:14
algorithm is just understanding the um the symbols here so i'm going to have a vector of resources so this is a
58:21
vector it's going to be a comma a comma separated list and for each resource r1 r2 r3 r4
58:29
i'm going to say how many of those resources are free so in this case here r1 and r2 are completely taken so we're
58:35
going to have 0 comma 0. we also have current requests from
58:40
thread x so if you notice for instance t1 is currently requesting an r1
58:46
so we're going to but it's not requesting an r2 so the request for uh t1 is going to be one comma zero the
58:53
allocation for t1 well it owns an r2 but not an r1 the allocation will be zero comma one
59:00
okay so these are just vectors of free resources numbers of free resources and how much
59:08
being requested and how much is allocated by each thread so if you can get past that then it's
59:14
very easy to do this we just do a list-based algorithm where we set the we say the total number of
59:20
available resources is the vector of free resources we put all the nodes um excuse me i
59:27
should say all the threads into the unfinished bin and then we're going to do we're going to start by saying well i'm done
59:33
equal true and then i'm going to go through and for every node that's in the unfinished bin i'm going to say well
59:39
is there enough nodes available enough resources available of each type
59:45
that i can get what i want my i'm currently requesting and if the answer is yes then i fini i figure out that as a
59:52
thread i can get all of those resources so i'm going to be able to finish and i'm going to renew remove the node from
59:59
the unfinished bin and then i'm going to add all of its resources back into the available pot
1:00:04
because i'm now done i'm going to say with that thread i'm going to say that i'm done with this algorithm i'm going to set it to false
1:00:11
and and then i'm going to go keep going and when i'm done with the first do loop i'm going to say gee did was there any
1:00:18
thread that finished as a result of going through if the answer is no then i'm done
1:00:24
and as a result i've got some nodes left and unfinished and i'm deadlocked because there's no way to finish this
1:00:30
on the other hand if i did pull a thread out in that pass i'd go back and try it again and i just keep looping
1:00:36
as long as threads are finishing and if i eventually finish everybody then i know there's no deadlock okay so
1:00:43
how do i know there's no deadlock because there is a path where threads can complete one at a time
1:00:50
and will eventually everybody will be finished and i won't exceed the uh the total resources in the system and
1:00:57
each thread as it finishes puts the resources back in the pot and um and then potentially those can be
1:01:04
used by other threads okay and if i did that let's see
1:01:09
um so the question here uh is basically
1:01:15
this is all fine and dandy but is it possible that uh we could have a situation where
1:01:22
one thread gets a resource uh and as a result uh other threads can't
1:01:28
finish and you end up with deadlock i think that was the question and the answer here is if you notice this deadlock algorithm is very careful
1:01:36
okay it's saying if a thread can get all of the things it needs
1:01:43
all of it right all of its requested remaining resources it can get them all at once
1:01:48
then i'll declare it finished and put all of its resources back in the pot and then as a result i
1:01:54
haven't prevented anybody else from running all i've done is freed up my own resources which they might potentially use
1:02:00
okay so this particular deadlock detection algorithm is saying is there any path that i could take
1:02:08
through the threads that would let them all finish okay did that answer
1:02:13
that question great so
1:02:20
um how do we so we can detect deadlock but how do we deal with it oh by the way
1:02:26
um can anybody tell me if i run this algorithm and i see
1:02:32
there is no deadlock according to this algorithm does that mean that all threads will finish
1:02:46
i've got both yes and no on here okay anybody want to argue both with a
1:02:54
question mark nobody wants to argue okay so the answer to this is no
1:03:00
but it's not the fault of the algorithm okay you got to be careful about what is this algorithm telling me it's telling
1:03:05
me that if the threads are asking for resources they need they use the
1:03:11
resources they free them up then other threads can go forward we're all happy but if a thread goes into an
1:03:17
infinite loop or something else happens or doesn't free up the resources for some reason
1:03:23
then this algorithm really doesn't tell you anything right so this this algorithm is assuming that the threads are
1:03:29
really just requesting resources and freeing them up and not doing anything else stupid like going into an infinite
1:03:35
loop okay so or asking for more things than they originally said they need
1:03:41
okay so this is this is a very restricted algorithmic algorithmic result here okay
1:03:48
now how should a system deal once it's discovered deadlock okay we have four approaches here that i wanted to mention
1:03:54
one is deadlock prevention so this is a situation where you write your code in a way so it
1:04:00
will never deadlock okay now i think i showed you that earlier uh when we talked about
1:04:06
removing the cycles from the network by uh eliminating certain directions of travel
1:04:13
right so that would be a prevention scenario deadlock recovery is a situation where
1:04:18
you let the deadlock happen and then you figure out how to recover from it okay that's the godzilla approach
1:04:24
deadlock avoidance is dynamically delay the resource request so that even though in principle you could get a
1:04:30
deadlock it doesn't happen and then finally i like to put this last
1:04:35
one out because this one's important and you should all know this exists i call this deadlock denial or deadlock denialism okay so this is
1:04:43
ignoring the possibility of deadlock and claiming that it never happens okay
1:04:48
and so modern operating systems
1:04:53
kind of make sure the system itself isn't involved in any deadlocks and then pretty much ignores
1:04:59
all the other deadlock and applications i like to call this the ostrich algorithm okay so
1:05:07
this is why sometimes you have something running and you got to reboot the operating system to
1:05:13
fix something okay that that oftentimes is because there's some deadlock that nobody uh planned for nobody
1:05:20
detected and nobody uh had any way to deal with other than just rebooting things
1:05:27
okay and unfortunately that's a much more common than you might think all right so let's talk a little bit
1:05:33
about uh prevention here for instance so one thing you can do is put infinite resources together
1:05:39
okay so that's uh you know infinites big right but what we're really saying is you include
1:05:44
enough resources so that no one ever really runs out of them doesn't have to be infinite just really big and you give the illusion of
1:05:51
infinite resources so a nice example of that might be virtual memory which under most circumstances appears
1:05:57
pretty big right um another somewhat less practical example might be the bay bridge with 12
1:06:03
000 lanes you never wait okay so that might be nice um never going to happen right infinite
1:06:10
disk space well we're pretty close to that in a lot of instances right you can buy a hundred terabyte um disk drive these
1:06:18
day these days uh that's using flash memory and that's pretty big okay um
1:06:26
you could decide to never share resources if you think about the cycles we're talking about earlier
1:06:32
cycles require that there's a resource that's being used by one person that is
1:06:39
uh needed by somebody else right if you never have any need for sharing you'll
1:06:44
never have any deadlock because you can't come up with a cycle another option would be never allow
1:06:50
waiting so notice what i'm doing here by the way is i'm removing uh one of those four requirements for
1:06:56
deadlock right so not allowing waiting is really how phone systems work
1:07:02
it used to be a lot more common it still happens occasionally where you try to call somebody and it the call
1:07:09
phone call actually works its way through the phone network but it gets blocked somewhere because there's not enough resources and what
1:07:14
happens is you get a busy signal what's really happening there is it's it bounces the call
1:07:20
and it assumes that you're going to retry by making the call again okay so what they've done there is
1:07:26
they've avoided deadlock in the network by pushing you off to doing a retry okay
1:07:33
this is actually the technique used in ethernet in some multi-processor networks where
1:07:39
you allow everybody to speak at once on a segment and if there's a collision then what
1:07:44
happens is you exponentially back off with some randomness and retry and as a result the the
1:07:50
problem goes away okay so this is a technique of random retry instead of a potential deadlock we'll
1:07:57
talk a little bit more about that uh later in the term um now it can be inefficient
1:08:02
if you don't use the right algorithms you know the goofy thing here is you consider driving to san francisco and the moment you hit a traffic jam
1:08:09
you're instantly teleported back home and have to retry that would be an example of uh you know
1:08:15
a retry mechanism that probably would never work because you could never make it through so if you're really going to reject and
1:08:22
force a retry there has to be some notion that there's going to be eventual success on that channel
1:08:29
okay here's an example of that virtually infinite resource i mentioned earlier
1:08:34
while we said this could deadlock if there's only two megabytes in the system but with virtual memory you have
1:08:40
effectively infinite space so everyone just gets to go through and you won't deadlock
1:08:45
okay now of course it's not actually infinite but it's certainly a lot larger than two megabytes
1:08:51
all right um how do we prevent deadlocks okay maybe this is a little more
1:08:56
interesting so you make all threads request everything they need at the beginning and then you check and see if
1:09:02
you've got enough resources and if you do you get to go and if you don't you don't
1:09:07
okay so if you think about that there'll never be a situation where you're in the middle of execution
1:09:13
you have some resources you're waiting for others you've just basically removed the weight portion of
1:09:20
that cycle okay the problem here of course is predicting the future as to what you need for resources and
1:09:27
you often end up overestimating um for the example if you need two chopsticks you request both at the same
1:09:33
time that may or may not work well i have imagined reserving the bay bridge that
1:09:38
wouldn't work too well either you don't leave home until you know no one's using any intersection between
1:09:44
here and where you want to go that actually works pretty well if you're traveling around uh
1:09:50
you know 1 30 at night across the bay bridge there might be enough channels there or lanes to know for sure
1:09:56
you're going to make it without being delayed you could force all
1:10:02
the threads to request resources in a particular order all right well this is more interesting so for instance to
1:10:07
prevent deadlock you always acquire x and then y and then z
1:10:12
okay so if you always acquire x and then y then z you can prove fairly simply that that'll
1:10:18
never deadlock because any deadlock involving those resources would have to be a cycle and therefore a
1:10:25
cycle would mean that some thread acquired something like z and then acquired x or z and then
1:10:31
acquired y so any actual cycle in a supposed deadlock would show you going backwards in your
1:10:38
acquisition and as a result can't happen because you always have to get x and then y and then c
1:10:44
and this by the way is exactly that dimension order uh routing that we talked about in uh multiple or trained networks
1:10:50
earlier right so here's an example so rather than what we showed earlier where you
1:10:56
could get x and then y and for a and then y and then x and b you just uh maybe acquire them both
1:11:03
okay so here we get both x and y both x y and x whatever either you get them all or you don't and
1:11:09
as a result there's no cycle that's the first thing i showed you the second was you
1:11:15
maybe get a lock around you grab z okay there's no cycle around z and if you happen to acquire z then
1:11:21
you can acquire what you want okay and that won't deadlock because
1:11:26
there's no cycle or here's the consistent order so rather
1:11:31
x then y y then x what you do is you always go x then y x then y okay and as a result it'll
1:11:38
never deadlock okay now does it matter which order the locks are released
1:11:45
like notice uh here we always go x then y x then y but here i'm releasing y then x
1:11:51
and here i'm releasing x then y doesn't matter
1:11:58
okay good it doesn't matter because the only thing i do with releasing is i'm letting people go forward i'm not
1:12:03
holding them up right so the releasing can be done in the order it's the acquisition that has to happen
1:12:09
in the uh the same order i will say however though typically you acquire them in one order
1:12:14
and you release them in another and that's just a way of making sure that um you've got a nice
1:12:19
clean pattern there and this is kind of what we looked at when we were talking about um
1:12:25
the finite buffer queue when we were when we were looking at semaphores a little while ago
1:12:31
train example right here is the fixing ordering of the channels you're always getting the x channel and then the y
1:12:37
channel and as a result there's no way to have a cycle because any cycle would show you having the y channel first and then the x channel
1:12:44
okay all right and this works in multiple dimen you know more dimensions you can have x y z w whatever as long as you act you get
1:12:52
them in a given order then you don't have deadlock
1:12:57
so how can we recover from deadlock so here you could terminate the thread force it to give up resources so that i
1:13:03
told you about the godzilla solution earlier you hold the dining lawyer in contempt
1:13:08
and take them away in handcuffs you make sure you get the chopsticks first
1:13:13
but it's not always possible because killing a thread holding a mutex would actually leave things inconsistent
1:13:18
and probably screw everything else up okay so taking things away is rarely a good thing
1:13:26
the one instance you could preempt resources without killing the thread but then again the resources think they have
1:13:32
the resources excuse me the threads think they have the resources exclusively you've just taken them away the thread's
1:13:38
going to not behave correctly okay so the one case where this actually works out well is when you
1:13:45
have enough information to do a full roll back or an abort and this is sort of the database idea where
1:13:52
before you grab your locks you have a checkpoint in the of the state and now you you go ahead
1:13:58
and start running if there's ever a deadlock you roll back to a time prior to the deadlock and you
1:14:03
restart things maybe with some randomness so the the deadlock doesn't happen again
1:14:08
this is a very common technique the databases can use because they can roll back to a
1:14:14
consistent state before they retry after detecting a deadlock so this is the one instance where
1:14:19
you can just back up and take resources away and retry it and make sure the deadlock doesn't
1:14:24
happen again okay many operating systems have other
1:14:30
options um but i will say that unix operations operating systems often use the
1:14:36
denialism technique so you know here's this other view of virtual memory so we said well we could
1:14:41
think of intimate space this isn't a problem if you look one level deep which will be appropriate one level deeper which will
1:14:48
be appropriate in the next lecture is we could say that what actually happens when we run out of memory for one of the
1:14:55
threads is we preempt that memory paging it out to disk
1:15:00
and giving it back later when we page it back in and as a result we can take memory from
1:15:06
one thread that means now dram physical memory give it to a different thread and everything's okay because
1:15:12
when we come back we grab the data and give it back to the thread we took it away from
1:15:18
and we don't let the thread look at it in the middle and so we find a way to suspend the use
1:15:24
save the state and avoid the deadlock okay and that's kind of what paging does
1:15:32
okay let's talk about avoiding okay so when a thread requests a
1:15:37
resource the operating system checks and sees would it result in deadlock if not it grants the
1:15:43
resource right away if so if there's going to be a deadlock it waits for the other threads to
1:15:48
release resources so this almost sounds good right so the idea is we we somehow look and we say will this
1:15:56
re will this thing we're giving it have a deadlock if so don't give it the resource
1:16:02
otherwise do and the issue here is let's show this example here's our thread a and b that could deadlock we
1:16:10
acquire x no deadlock there we acquire y there's no deadlock there there's no cycle
1:16:16
we acquire y still no deadlock okay now notice at this point thread a is
1:16:22
blocking because it's trying to acquire a resource b has we still don't have a cycle because b is happily running here
1:16:30
we say oh if we acquire x we're going to have a cycle and therefore deadlock so we'll wait
1:16:36
problem is it's already too late because there's already impossible it's already impossible for this
1:16:42
situation to resolve even though there isn't a cycle at the moment you try to acquire x
1:16:47
so we have to do something a little better here and so here i'm going to introduce three
1:16:52
states there's the safe state which is the system can delay the resource acquisition to prevent deadlock
1:16:58
so this is a situation where we can make forward progress and we won't deadlock there's deadlock where we're in trouble
1:17:04
right and we already have a cycle and then there's an unsafe state where there's no deadlock yet
1:17:10
but threads could request resources in a pattern that will unavoidably lead to
1:17:16
deadlock that's what we had in that previous slide we already had an unsafe state
1:17:21
okay and actually the deadlock states considered unsafe as well because once you're dead locked it's not safe
1:17:27
so deadlock avoidance is preventing the system from reaching an unsafe state
1:17:33
so how do we do that so for instance when a thread requests a resource os checks and sees if it would result in
1:17:39
an unsafe state if not it grants the resource if so it waits so how this changes our example
1:17:46
is thread a grabs x everybody's good thread b tries to grab
1:17:53
y but we look and we say oh if we acquire y we're already down the path to an in
1:18:00
an inevitable deadlock and therefore thread b is not even allowed to acquire y okay if we could
1:18:06
come up with that then what happens is b goes you know to sleep and it's stalled a goes on to acquire y
1:18:14
does its thing releases the two at that point b gets released and now we're good to go and we don't have any deadlock
1:18:21
so this algorithm that i've sort of implied has somehow kept us in
1:18:27
a safe state and therefore we don't deadlock and that's kind of what we'd like to do so we have something called the baker's
1:18:33
algorithm so toward the right idea is state the maximum at the beginning of resources you need
1:18:39
and you allow a particular thread to proceed only if the total number of resources minus the
1:18:45
number i'm requesting still says that there's an amount it's uh
1:18:50
the remaining is greater than the maximum that anybody needs so we take the current available minus
1:18:56
what i'm asking for and as long as what's left is greater or than or equal to the max than anybody will need
1:19:02
i'm good to go why is that okay well that says basically that um gee even though i've been given these
1:19:09
resources there's always somebody that can complete so this is
1:19:15
not quite what we want this is a little too uh conservative okay instead the
1:19:21
banker's algorithm is a little less conservative and it's going to let you ask for resources free them ask for them
1:19:27
free them so on and what we're going to do is every time somebody asks for a request
1:19:33
we'll grant it as long as there is some way for the threads to run such that they
1:19:38
will complete without a deadlock so we only run we only grant a resource
1:19:44
if there's some way to complete without a deadlock so the technique here is to pretend that
1:19:50
we grant the resource that's being asked for and then we run the deadlock detection algorithm
1:19:56
and in that case we're going to substitute this which is say take the maximum that anybody wants
1:20:02
take the maximum that a given node wants minus the amount they have um and see whether it's less than what's
1:20:08
available and we're going to replace that for what we asked about earlier which is um you know
1:20:14
seeing whether what we're requesting is is uh less than what's available okay and so here notice that in this
1:20:21
deadlock detection algorithm we're going to say that for every node instead of if requesting the amount
1:20:26
we're requesting is less than what's available we're going to say if we take the maximum we need minus how
1:20:34
much we have is less than what's available then we're gonna we can finish
1:20:39
okay and so this is like that simulation that i talked about earlier where for um we temporarily grant the thread
1:20:47
that's asking for something and then we go through and we say is there a way to let some thread finish
1:20:52
and then let some other thread finish and so on as long as there's still a path through that'll allow the thread some the whole
1:20:59
set of threads to finish we're not dead locked and we're still in a safe state okay so basically that algorithm as
1:21:08
simple as it is which is substituting this into the deadlock freedom algorithm keeps the system in a safe state which
1:21:14
says there's always a sequence t1 t2 to tn where t1 completes then t2
1:21:20
completes and so on there's always a path out even if i pretend to give the resource
1:21:27
and if that's true then i go ahead and give the resource okay so the way you need to think of the banker's algorithm
1:21:33
i realize i'm a little bit low on time just give me a few more minutes the way to think about the bankers algorithm is that it's a simulation
1:21:42
of what would happen if i granted the resource to the thread would i still be able to find a way out such that every
1:21:48
thread completes and if the answer is yes then go ahead and give it okay and this is a an
1:21:56
actual algorithm that we could run on every acquire and release of every resource
1:22:02
that would prevent us from deadlocking and it would actually uh do that run that uh example i showed
1:22:07
you earlier where we grab x and grab y and then the other case grab y and then x this particular algorithm would actually
1:22:14
prevent that from ever deadlocking because thread b would be forced to wait until thread a was fully done
1:22:20
in that instance okay so in some examples here if you think through the banker's
1:22:25
algorithm what you would find is that a safe state which is one that doesn't uh cause deadlock is if when you try to
1:22:32
grab a chopstick either it's not the last chopstick or it is the last chopstick but somebody
1:22:38
else has two if either of these conditions are correct then um you're going to still be in a safe state
1:22:44
and you can allow that chopstick to be acquired where this gets a little bit amusing as you can imagine the k-handed lawyer case
1:22:51
which is you don't allow if it's the last one and no one would have k chopsticks or it's the second to last one and no
1:22:57
one would have k minus one and so on okay
1:23:03
so um we're about done here yes uh we can actually do a a uh k-handed lawyers case so i want to
1:23:10
pause for two seconds about whether there's any uh questions here and then i'm gonna finish up
1:23:18
so you have to think about the the way you think about the banker's algorithm is on every acquire or release of every
1:23:24
resource you pretend that you give that resource the colonel does this it pretends it's
1:23:30
going to give that resource it runs that special deadlock detection algorithm and says am i going to go into deadlock
1:23:36
uh or is there a path out of this if there's a path out then it will grant the resource if there
1:23:41
is not a path out then it won't grant the resource and instead put that thread to sleep until there are enough
1:23:46
resources all right
1:23:52
good so the question of course is do most os
1:23:58
implement this so as i already told you unix essentially uses the ostrich approach or
1:24:03
uh deadlock denialism however uh if you care you can implement
1:24:10
this so there are some specialized os's that do do this the second thing that's kind of interesting is i think shown by this
1:24:17
page here which is you can use the the uh bankers algorithm to design a
1:24:25
way of accessing resources that won't deadlock so you can use the banker's algorithm
1:24:30
as a way of designing how you go about asking for resources in a given
1:24:38
in a given application then you don't actually need the banker's algorithm running live because
1:24:43
you've set it up so that it's running as part of your actual application okay and you could even run a banker's
1:24:51
algorithm library inside of a cell of an application instead of the operating system
1:24:59
all right so we talked about the four conditions for deadlocks we talked about mutual exclusion
1:25:04
which is that when you get a resource you have exclusively hold of it hold in weight which is i
1:25:10
hold on to other resources while i'm waiting for ones that i'm looking for no preemption says i can't take
1:25:15
resources away circular weight there's at least a cycle in the system all four of these are required these are
1:25:21
necessary but not sufficient for deadlock we talked about techniques for addre for
1:25:27
uh basically addressing the deadlock problem we can either prevent it by writing our
1:25:32
code so it won't be prone to deadlock we can um that includes things like
1:25:38
dimension order routing we can avoid uh so that's avoiding the deadlock we can
1:25:43
recover from deadlocks by uh rolling back we can avoid it entirely
1:25:49
by something like the bankers algorithm or we can totally ignore the possibility which unfortunately a lot of things do
1:25:56
all right i think we're good for today um and look forward to the results of the midterm
1:26:03
grading coming out and again it was a little longer than we intended we apologize for that i
1:26:08
hope you have a great weekend um everybody i hope that you can get outside and that the air improves a
1:26:14
little bit
1:26:19
ciao